#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Complete PDF Translation Pipeline with Claude API
ì™„ì „í•œ PDF ë²ˆì—­ íŒŒì´í”„ë¼ì¸ (ëª¨ë“  ì²­í¬ ë²ˆì—­)
"""
import sys
import os
from pathlib import Path
from typing import List, Optional
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict

# Set encoding for Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


def get_api_key() -> Optional[str]:
    """Get Claude API key"""
    return os.getenv('ANTHROPIC_API_KEY')


# ëª¨ë¸ë³„ ê°€ê²©(USD per 1M tokens). í•„ìš” ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸° ì§€ì›.
# - ê³µì‹ ê°€ê²©: https://www.anthropic.com/pricing
# - Claude Haiku 4.5: $1.00/M input, $5.00/M output (2025-01-01 ê¸°ì¤€)
# - í™˜ê²½ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸°: CLAUDE_HAIKU_45_INPUT_MTOK, CLAUDE_HAIKU_45_OUTPUT_MTOK
PRICING_USD_PER_MTOK = {
    "claude-haiku-4-5-20251001": {
        "input": float(os.getenv("CLAUDE_HAIKU_45_INPUT_MTOK", "1.00")),
        "output": float(os.getenv("CLAUDE_HAIKU_45_OUTPUT_MTOK", "5.00")),
    },
    # ë‹¤ë¥¸ ëª¨ë¸ ì¶”ê°€ ê°€ëŠ¥
    "claude-3-5-sonnet-20241022": {
        "input": float(os.getenv("CLAUDE_SONNET_35_INPUT_MTOK", "3.00")),
        "output": float(os.getenv("CLAUDE_SONNET_35_OUTPUT_MTOK", "15.00")),
    },
}

def _get_model_pricing(model_name: str) -> dict:
    """ëª¨ë¸ë³„ ê°€ê²© ì •ë³´ë¥¼ ë°˜í™˜. ë¯¸ë“±ë¡ ëª¨ë¸ì€ 0ìœ¼ë¡œ ì±„ì›Œ ë°˜í™˜."""
    return PRICING_USD_PER_MTOK.get(model_name, {"input": 0.0, "output": 0.0})


def extract_glossary(text: str, api_key: str, sample_size: int = 30000) -> dict:
    """
    ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ìš©ì–´ ì¶”ì¶œ ë° ë²ˆì—­
    
    ë¬¸ì„œë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ë¶„ì•¼ë¥¼ íŒŒì•…í•˜ê³  í•µì‹¬ ì „ë¬¸ ìš©ì–´ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì´ë¥¼ í†µí•´ ì–´ë–¤ ë¶„ì•¼ì˜ ë¬¸ì„œê°€ ì™€ë„ ì¼ê´€ëœ ìš©ì–´ ë²ˆì—­ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    
    ì „ëµ:
    - í…ìŠ¤íŠ¸ ìƒ˜í”Œë§: ì²˜ìŒ 15k + ì¤‘ê°„ 10k + ë 5k chars
    - Haiku ëª¨ë¸ ì‚¬ìš©ìœ¼ë¡œ ë¹„ìš© ìµœì†Œí™” (~$0.01)
    - JSON ì‘ë‹µìœ¼ë¡œ íŒŒì‹± ê°„í¸í™”
    
    Args:
        text: ì „ì²´ í…ìŠ¤íŠ¸
        api_key: Anthropic API í‚¤
        sample_size: ìƒ˜í”Œë§í•  ì´ í¬ê¸° (ê¸°ë³¸ 30000)
    
    Returns:
        {
            "domain": "ë¬¸ì„œ ë¶„ì•¼",
            "key_terms": {"ì˜ë¬¸": "í•œê¸€", ...}
        }
    """
    print(f"[ANALYZING] Extracting key terms from document...", flush=True)
    
    # ìƒ˜í”Œë§ ì „ëµ: ì²˜ìŒ, ì¤‘ê°„, ëì—ì„œ ê³ ë¥´ê²Œ
    total_len = len(text)
    sample = (
        text[:15000] +  # ì²˜ìŒ (ë„ì…ë¶€, ì£¼ìš” ê°œë…)
        text[total_len//2:total_len//2+10000] +  # ì¤‘ê°„ (í•µì‹¬ ë‚´ìš©)
        text[-5000:]  # ë (ê²°ë¡ , ìš”ì•½)
    )
    
    prompt = f"""ì´ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì „ë¬¸ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ã€ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸ ìƒ˜í”Œã€‘
{sample}

ã€ì‘ì—…ã€‘
1. ë¬¸ì„œì˜ ë¶„ì•¼/ì£¼ì œë¥¼ íŒŒì•…í•˜ì„¸ìš” (ì˜ˆ: startup, medicine, law, technology, finance ë“±)
2. ìì£¼ ë“±ì¥í•˜ê±°ë‚˜ ì¤‘ìš”í•œ ì „ë¬¸ ìš©ì–´ 20-30ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”
3. ê° ìš©ì–´ì˜ ì ì ˆí•œ í•œêµ­ì–´ ë²ˆì—­ì„ ì œì‹œí•˜ì„¸ìš”

ã€ë²ˆì—­ ì›ì¹™ã€‘
- í•´ë‹¹ ë¶„ì•¼ì—ì„œ í†µìš©ë˜ëŠ” ë²ˆì—­ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
- ì—†ìœ¼ë©´ ìŒì°¨ ë˜ëŠ” ì˜ì—­
- ì•½ì–´ëŠ” ê°€ëŠ¥í•˜ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì˜ˆ: CEO, MVP, API)
- ì¼ê´€ì„±ì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤

ã€ì¤‘ìš” ì˜ˆì‹œã€‘
- startup ë¶„ì•¼: "term sheet" â†’ "í…€ì‹œíŠ¸" (âš ï¸ "ì´ìš©ì•½ê´€" ì•„ë‹˜!)
- medicine: "hypertension" â†’ "ê³ í˜ˆì••"
- law: "plaintiff" â†’ "ì›ê³ "

ã€ì¶œë ¥ í˜•ì‹ã€‘
JSONë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´):
{{
  "domain": "ë¶„ì•¼ëª…",
  "key_terms": {{
    "ì˜ë¬¸ìš©ì–´1": "í•œê¸€ë²ˆì—­1",
    "ì˜ë¬¸ìš©ì–´2": "í•œê¸€ë²ˆì—­2"
  }}
}}"""

    try:
        from anthropic import Anthropic
        client = Anthropic(api_key=api_key)
        
        response = client.messages.create(
            model="claude-haiku-4-5-20251001",  # ì €ë ´í•œ ëª¨ë¸
            max_tokens=2000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # JSON íŒŒì‹±
        result_text = response.content[0].text
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0]
        
        glossary = json.loads(result_text.strip())
        
        print(f"[OK] Glossary extracted successfully", flush=True)
        return glossary
        
    except Exception as e:
        print(f"[WARNING] Glossary extraction failed: {e}", flush=True)
        print(f"[INFO] Proceeding without custom glossary", flush=True)
        return {"domain": "unknown", "key_terms": {}}


def extract_pdf(pdf_path):
    """
    Extract text from PDF with progress tracking

    ì´ í•¨ìˆ˜ëŠ” PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤:
    1. PDF ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ (í˜ì´ì§€ ìˆ˜, ì œëª© ë“±)
    2. ê° í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    3. ì§„í–‰ë¥  í‘œì‹œ (ë§¤ 5í˜ì´ì§€ë§ˆë‹¤)
    4. ì „ì²´ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë°˜í™˜

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ

    Returns:
        tuple: (ì „ì²´_í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°, í˜ì´ì§€_ëª©ë¡) ë˜ëŠ” ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ (None, None, None)
    """
    try:
        import pdfplumber

        with pdfplumber.open(pdf_path) as pdf:
            text = ""
            metadata = pdf.metadata
            pages = []

            print(f"[PDF Info]")
            print(f"  Pages: {len(pdf.pages)}")
            if metadata:
                print(f"  Title: {metadata.get('Title', 'N/A')}")
            print()
            print(f"[EXTRACTING] Processing {len(pdf.pages)} pages...")
            print()

            for i, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
                pages.append(page_text if page_text else "")
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ (ë§¤ 5í˜ì´ì§€ë§ˆë‹¤)
                if i % 5 == 0 or i == len(pdf.pages):
                    progress = (i / len(pdf.pages)) * 100
                    print(f"  [{i:3d}/{len(pdf.pages)}] {progress:5.1f}% complete", flush=True)
            
            print()
            return text, metadata, pages

    except ImportError:
        print("[ERROR] pdfplumber not installed: pip install pdfplumber")
        return None, None, None


def chunk_text(text, chunk_size=5000, overlap_sentences=2):
    """
    ìŠ¤ë§ˆíŠ¸ ì²­í‚¹: ë¬¸ì¥ ê²½ê³„ë¥¼ ê°ì§€í•˜ì—¬ ì˜ë¯¸ ë‹¨ìœ„ ê¸°ë°˜ ë¶„í• 

    ì´ í•¨ìˆ˜ëŠ” ë‹¨ìˆœ í¬ê¸° ê¸°ë°˜ì´ ì•„ë‹Œ ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤:

    1. ë¬¸ì¥ ê²½ê³„ ê°ì§€ (ì •ê·œì‹ ê¸°ë°˜):
       - `.`, `?`, `!`ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
       - ì•½ì–´ (Mr., U.S. ë“±) ë° URL ë³´ì¡´
       - ì •ê·œì‹: r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s+'

    2. ì²­í¬ ìƒì„± (chunk_size ê¸°ì¤€):
       - ê° ì²­í¬ëŠ” chunk_size(ê¸°ë³¸ 5000ì)ë¥¼ ëª©í‘œë¡œ í•¨
       - ë¬¸ì¥ ê²½ê³„ì—ì„œë§Œ ë¶„í• í•˜ì—¬ ì˜ë¯¸ ë³´ì¡´

    3. ì»¨í…ìŠ¤íŠ¸ ì˜¤ë²„ë© (2ë¬¸ì¥):
       - ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ Nê°œ ë¬¸ì¥ì„ ìƒˆ ì²­í¬ ì‹œì‘ì— í¬í•¨
       - ë²ˆì—­ ì¼ê´€ì„± ë³´ì¥ ë° ì²­í¬ ê²½ê³„ ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬
       - ê° ì²­í¬ëŠ” {'text': '...', 'overlap': '...'} í˜•ì‹ìœ¼ë¡œ ë°˜í™˜

    ì„±ëŠ¥:
    - 11ê°œ ì²­í¬ ìƒì„± (50,898ì ë¬¸ì„œ): <1ì´ˆ
    - ì˜¤ë²„ë©ìœ¼ë¡œ ì¸í•œ í¬ê¸° ì¦ê°€: ~5-10%

    Args:
        text (str): ë¶„í• í•  í…ìŠ¤íŠ¸
        chunk_size (int): ê° ì²­í¬ì˜ ëª©í‘œ í¬ê¸° (ê¸°ë³¸ 5000ì)
        overlap_sentences (int): ì²­í¬ ê°„ ì˜¤ë²„ë© ë¬¸ì¥ ìˆ˜ (ê¸°ë³¸ 2)

    Returns:
        List[dict]: {'text': ì²­í¬_ë‚´ìš©, 'overlap': ì´ì „_ì»¨í…ìŠ¤íŠ¸} í˜•ì‹ì˜ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    print(f"[CHUNKING] Smart chunking with sentence boundaries...", flush=True)
    
    import re
    
    # ë¬¸ì¥ ë¶„ë¦¬ (ê°œì„ ëœ ì •ê·œì‹ - ì•½ì–´, URL ë“± ê³ ë ¤)
    sentence_pattern = r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s+'
    sentences = re.split(sentence_pattern, text)
    
    chunks = []
    current_chunk = []
    current_size = 0
    overlap_buffer = []  # ì˜¤ë²„ë©ì„ ìœ„í•œ ìµœê·¼ ë¬¸ì¥ ì €ì¥
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        sentence_size = len(sentence)
        
        # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆ ì²­í¬ ì‹œì‘
        if current_size + sentence_size > chunk_size and current_chunk:
            # í˜„ì¬ ì²­í¬ ì €ì¥
            chunk_text = " ".join(current_chunk)
            chunks.append({
                'text': chunk_text,
                'overlap': " ".join(overlap_buffer) if overlap_buffer else None
            })
            
            # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ Nê°œ ë¬¸ì¥ ì €ì¥
            overlap_buffer = current_chunk[-overlap_sentences:] if len(current_chunk) >= overlap_sentences else current_chunk[:]
            
            # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘)
            current_chunk = overlap_buffer[:] + [sentence]
            current_size = sum(len(s) for s in current_chunk)
        else:
            current_chunk.append(sentence)
            current_size += sentence_size
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk:
        chunk_text = " ".join(current_chunk)
        chunks.append({
            'text': chunk_text,
            'overlap': " ".join(overlap_buffer) if overlap_buffer and len(chunks) > 0 else None
        })
    
    print(f"[OK] Created {len(chunks)} chunks with context overlap", flush=True)
    return chunks


def translate_with_claude(
    text: str,
    source_lang: str = "English",
    target_lang: str = "Korean",
    api_key: Optional[str] = None,
    chunk_num: int = 0,
    total_chunks: int = 0,
    context: Optional[str] = None,
    glossary: Optional[dict] = None
) -> Optional[dict]:
    """
    ì „ë¬¸ ë²ˆì—­ê°€ ìˆ˜ì¤€ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ Claude API ê¸°ë°˜ ë²ˆì—­

    ì´ í•¨ìˆ˜ëŠ” 20ë…„ ê²½ë ¥ì˜ ì¶œíŒ ë²ˆì—­ê°€ í˜ë¥´ì†Œë‚˜ë¥¼ í™œìš©í•˜ì—¬ ê³ í’ˆì§ˆ ë²ˆì—­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

    1. ì „ë¬¸ ë²ˆì—­ê°€ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:
       - í˜ë¥´ì†Œë‚˜: 20ë…„ ê²½ë ¥ ì¶œíŒ ë²ˆì—­ê°€ (ë¹„ì¦ˆë‹ˆìŠ¤/ìŠ¤íƒ€íŠ¸ì—… ë¶„ì•¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë‹¤ìˆ˜)
       - í†¤: ì •ì¤‘í•˜ê³  ì¹œê·¼í•œ ì¡´ëŒ“ë§ (ê²½ì–´ì²´)
       - ëŒ€ìƒ ë…ì: ìŠ¤íƒ€íŠ¸ì—…/ë¹„ì¦ˆë‹ˆìŠ¤ì— ê´€ì‹¬ ìˆëŠ” ì§€ì  ë…ìì¸µ

    2. 5ê°€ì§€ ë²ˆì—­ ì² í•™:
       a) ì˜ë¯¸ì˜ ì¶©ì‹¤ì„± > ì§ì—­
          - ì›ë¬¸ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ ë‰˜ì•™ìŠ¤ ì™„ë²½ ì „ë‹¬
          - ë‹¨ì–´ í•˜ë‚˜í•˜ë‚˜ë³´ë‹¤ ë¬¸ì¥ ì „ì²´ ì˜ë„ íŒŒì•…
          - ì˜ì–´ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ì§€ ë§ê³  í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ìƒê°

       b) ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ (ë²ˆì—­ì²´ ì œê±°)
          - "~ë˜ì–´ì§€ë‹¤", "~ì— ì˜í•´", "ê²ƒì´ë‹¤" ë“± ê¸ˆì§€
          - ëŠ¥ë™íƒœ ìš°ì„ , ìˆ˜ë™íƒœëŠ” í•„ìš”í•œ ê²½ìš°ì—ë§Œ

       c) ì½ê¸° ì‰¬ìš´ ë¬¸ì¥
          - í•œ ë¬¸ì¥ì— í•˜ë‚˜ì˜ í•µì‹¬ ì•„ì´ë””ì–´
          - ê¸´ ë¬¸ì¥ì€ 2-3ê°œë¡œ ë¶„ë¦¬
          - ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°

       d) ë§¥ë½ê³¼ íë¦„
          - ë¬¸ì¥ ê°„ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°
          - ì•ë’¤ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ë²ˆì—­
          - ë‹¨ë½ì˜ ì „ì²´ íë¦„ ìœ ì§€

       e) í†¤ê³¼ ë‰˜ì•™ìŠ¤ ë³´ì¡´
          - ì €ìì˜ ê°œì¸ì  ì´ì•¼ê¸°ëŠ” ë”°ëœ»í•¨
          - í†µê³„/ë°ì´í„°ëŠ” ê°ê´€ì ì„
          - ì¡°ì–¸/êµí›ˆì€ ì§ì„¤ì ì´ë©´ì„œ ì‹¤ìš©ì 

    3. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë²ˆì—­:
       - ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ì„ ì°¸ê³ ì •ë³´ë¡œ ì œê³µ
       - ë²ˆì—­ ì¼ê´€ì„± ë³´ì¥ (ìš©ì–´, í†¤, êµ¬ì¡°)
       - ì²­í¬ ê²½ê³„ì˜ ì–´ìƒ‰í•¨ ì œê±°

    4. 30ê°œ í•µì‹¬ ìš©ì–´ ì‚¬ì „ ë‚´ì¥:
       - startup â†’ ìŠ¤íƒ€íŠ¸ì—…
       - founder â†’ ì°½ì—…ì
       - venture capital â†’ ë²¤ì²˜ìºí”¼íƒˆ (VC í—ˆìš©)
       - ... ë“± 30ê°œ ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´

    5. ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸:
       - ìì—°ìŠ¤ëŸ¬ìš´ ë°œìŒ
       - ë²ˆì—­ì²´ í‘œí˜„ ì œê±°
       - í•œêµ­ ë…ìì˜ ì´í•´ ìš©ì´ì„±
       - ì „ë¬¸ì„±ê³¼ ê°€ë…ì„± ê· í˜•
       - ì›ë¬¸ì˜ í†¤ê³¼ ë‰˜ì•™ìŠ¤ ë³´ì¡´

    ì„±ëŠ¥:
    - ì²­í¬ë‹¹ ì†Œìš”ì‹œê°„: 4-6ì´ˆ (ë³‘ë ¬ ì²˜ë¦¬ ì‹œ)
    - ëª¨ë¸: claude-haiku-4-5-20251001
    - ìµœëŒ€ í† í°: 64,000

    Args:
        text (str): ë²ˆì—­í•  í…ìŠ¤íŠ¸
        source_lang (str): ì›ë¬¸ ì–¸ì–´ (ê¸°ë³¸ "English")
        target_lang (str): ëª©í‘œ ì–¸ì–´ (ê¸°ë³¸ "Korean")
        api_key (Optional[str]): Anthropic API í‚¤
        chunk_num (int): í˜„ì¬ ì²­í¬ ë²ˆí˜¸ (ì§„í–‰ë¥  í‘œì‹œìš©)
        total_chunks (int): ì „ì²´ ì²­í¬ ìˆ˜ (ì§„í–‰ë¥  í‘œì‹œìš©)
        context (Optional[str]): ì´ì „ ì²­í¬ì˜ ì˜¤ë²„ë© í…ìŠ¤íŠ¸ (ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ìš©)

    Returns:
        Optional[dict]: {
            'text': ë²ˆì—­ë¬¸,
            'usage': {'input_tokens': int, 'output_tokens': int, 'total_tokens': int},
            'model': str
        } ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
    """
    if not api_key:
        return None

    try:
        from anthropic import Anthropic

        client = Anthropic(api_key=api_key)
        model_name = "claude-haiku-4-5-20251001"

        # ìš©ì–´ì§‘ ì„¹ì…˜ ìƒì„±
        glossary_section = ""
        if glossary and glossary.get("key_terms"):
            terms = glossary["key_terms"]
            domain = glossary.get("domain", "unknown")
            glossary_section = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ì´ ë¬¸ì„œì˜ í•µì‹¬ ìš©ì–´ì§‘ - ë°˜ë“œì‹œ ì¤€ìˆ˜!ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ë¬¸ì„œ ë¶„ì•¼: {domain}

í•„ìˆ˜ ìš©ì–´ ë²ˆì—­ (ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”):
"""
            for eng, kor in list(terms.items())[:30]:  # ìµœëŒ€ 30ê°œ
                glossary_section += f"{eng} â†’ {kor}\n"
            
            glossary_section += """
âš ï¸ ìœ„ ìš©ì–´ë“¤ì€ ì´ ë¬¸ì„œ ì „ì²´ì—ì„œ ì¼ê´€ë˜ê²Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤!
âš ï¸ ê°™ì€ ìš©ì–´ë¥¼ ë‹¤ë¥´ê²Œ ë²ˆì—­í•˜ì§€ ë§ˆì„¸ìš”!

"""
        
        # í”„ë¡œ ë²ˆì—­ê°€ ìˆ˜ì¤€ì˜ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì „ë¬¸ ì¶œíŒ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ë¥¼ ë‹¤ìˆ˜ ë²ˆì—­í–ˆìœ¼ë©°, ë…ìë“¤ë¡œë¶€í„° "ì›ë¬¸ë³´ë‹¤ ë” ì˜ ì½íŒë‹¤"ëŠ” í‰ê°€ë¥¼ ë°›ìŠµë‹ˆë‹¤.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ë²ˆì—­ ì² í•™ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. ì˜ë¯¸ì˜ ì¶©ì‹¤ì„± > ì§ì—­
   - ì›ë¬¸ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ ë‰˜ì•™ìŠ¤ë¥¼ ì™„ë²½íˆ ì „ë‹¬
   - ë‹¨ì–´ í•˜ë‚˜í•˜ë‚˜ë³´ë‹¤ ë¬¸ì¥ ì „ì²´ì˜ ì˜ë„ë¥¼ íŒŒì•…
   - ì˜ì–´ì˜ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ì§€ ë§ê³ , í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ìƒê°í•˜ì—¬ í‘œí˜„

2. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ (ë²ˆì—­ì²´ ì œê±°)
   - "~ë˜ì–´ì§€ë‹¤", "~ì— ì˜í•´", "ê²ƒì´ë‹¤" ë“± ë²ˆì—­ì²´ í‘œí˜„ ì ˆëŒ€ ê¸ˆì§€
   - ëŠ¥ë™íƒœ ìš°ì„ , ìˆ˜ë™íƒœëŠ” í•„ìš”í•œ ê²½ìš°ì—ë§Œ
   - í•œêµ­ì¸ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ

3. ì½ê¸° ì‰¬ìš´ ë¬¸ì¥
   - í•œ ë¬¸ì¥ì— í•˜ë‚˜ì˜ í•µì‹¬ ì•„ì´ë””ì–´
   - ê¸´ ë¬¸ì¥ì€ 2-3ê°œë¡œ ë¶„ë¦¬
   - ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°
   - ë¦¬ë“¬ê° ìˆê²Œ

4. ë§¥ë½ê³¼ íë¦„
   - ë¬¸ì¥ ê°„ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°
   - ì•ë’¤ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ë²ˆì—­
   - ë‹¨ë½ì˜ ì „ì²´ íë¦„ ìœ ì§€

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ìŠ¤íƒ€ì¼ ê°€ì´ë“œã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… í†¤: ì •ì¤‘í•˜ê³  ì¹œê·¼í•œ ì¡´ëŒ“ë§ (ê²½ì–´ì²´: ~í•©ë‹ˆë‹¤, ~ìŠµë‹ˆë‹¤)
âœ… ëŒ€ìƒ: í•´ë‹¹ ë¶„ì•¼ì— ê´€ì‹¬ ìˆëŠ” ì§€ì  ë…ì
âœ… ë¬¸ì²´: ì „ë¬¸ì ì´ë©´ì„œë„ ì‰½ê²Œ ì½íˆëŠ” êµì–‘ì„œ ìŠ¤íƒ€ì¼

{glossary_section}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ë²ˆì—­ ì˜ˆì‹œ - ë‚˜ìœ vs ì¢‹ì€ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì›ë¬¸: "I was 25 years old and completely panicked, but I'm a terrible liar."

âŒ ë‚˜ìœ ë²ˆì—­ (ë²ˆì—­ì²´):
"ì €ëŠ” 25ì„¸ì˜€ê³  ì™„ì „íˆ íŒ¨ë‹‰ ìƒíƒœì— ìˆì—ˆìŠµë‹ˆë‹¤ë§Œ, ì €ëŠ” ê±°ì§“ë§ì„ í•˜ëŠ” ê²ƒì— ì„œíˆ° ì‚¬ëŒì…ë‹ˆë‹¤."

âœ… ì¢‹ì€ ë²ˆì—­ (ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´):
"ë‹¹ì‹œ ìŠ¤ë¬¼ë‹¤ì„¯ì´ì—ˆë˜ ì €ëŠ” ì™„ì „íˆ ë‹¹í™©í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì €ëŠ” ê±°ì§“ë§ì„ ì •ë§ ëª»í•©ë‹ˆë‹¤."

---

ì›ë¬¸: "That's when I realized we had to pivot."

âŒ ë‚˜ìœ ë²ˆì—­:
"ê·¸ê²ƒì€ ìš°ë¦¬ê°€ í”¼ë²—ì„ í•´ì•¼ë§Œ í–ˆë‹¤ëŠ” ê²ƒì„ ì œê°€ ê¹¨ë‹¬ì•˜ì„ ë•Œì˜€ìŠµë‹ˆë‹¤."

âœ… ì¢‹ì€ ë²ˆì—­:
"ê·¸ë•Œ ê¹¨ë‹¬ì•˜ìŠµë‹ˆë‹¤. ë°©í–¥ì„ ë°”ê¿”ì•¼ í•œë‹¤ëŠ” ê²ƒì„ìš”."

---

ì›ë¬¸: "The key to success in B2B sales is building relationships."

âŒ ë‚˜ìœ ë²ˆì—­:
"B2B ì˜ì—…ì—ì„œì˜ ì„±ê³µì˜ ì—´ì‡ ëŠ” ê´€ê³„ë“¤ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."

âœ… ì¢‹ì€ ë²ˆì—­:
"B2B ì˜ì—…ì—ì„œ ì„±ê³µí•˜ë ¤ë©´ ê´€ê³„ êµ¬ì¶•ì´ í•µì‹¬ì…ë‹ˆë‹¤."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ë²ˆì—­í•  í…ìŠ¤íŠ¸ã€‘ (Chunk {chunk_num}/{total_chunks})
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{"" if not context else f'''
âš ï¸ ì´ì „ ë§¥ë½ (ì°¸ê³ ìš© - ë²ˆì—­í•˜ì§€ ë§ˆì„¸ìš”):
---
{context}
---

ğŸ’¡ ìœ„ ë‚´ìš©ì€ ì´ë¯¸ ë²ˆì—­ëœ ë¶€ë¶„ì…ë‹ˆë‹¤. íë¦„ê³¼ ë§¥ë½ì„ ì´í•´í•˜ëŠ” ë°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

'''}
ğŸ“ ì´ì œ ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•˜ì„¸ìš”:
---
{text}
---

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ë²ˆì—­í•˜ê¸° ì „ì—:
1. ë‹¨ë½ ì „ì²´ë¥¼ ì½ê³  ë§¥ë½ì„ íŒŒì•…í–ˆëŠ”ê°€?
2. ì €ìê°€ ì „ë‹¬í•˜ê³ ì í•˜ëŠ” í•µì‹¬ ë©”ì‹œì§€ë¥¼ ì´í•´í–ˆëŠ”ê°€?

ë²ˆì—­í•œ í›„ì—:
1. ì†Œë¦¬ ë‚´ì–´ ì½ì—ˆì„ ë•Œ ìì—°ìŠ¤ëŸ¬ìš´ê°€?
2. ë²ˆì—­ì²´ í‘œí˜„("~ë˜ì–´ì§€ë‹¤", "~ê²ƒì´ë‹¤" ë“±)ì´ ì—†ëŠ”ê°€?
3. í•œêµ­ ë…ìê°€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ”ê°€?
4. ì „ë¬¸ì„±ê³¼ ê°€ë…ì„±ì˜ ê· í˜•ì´ ë§ëŠ”ê°€?
5. ì›ë¬¸ì˜ í†¤ê³¼ ë‰˜ì•™ìŠ¤ê°€ ì‚´ì•„ìˆëŠ”ê°€?

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ì£¼ì„ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤."""

        message = client.messages.create(
            model=model_name,
            max_tokens=64000,
            messages=[{"role": "user", "content": prompt}]
        )

        result_text = message.content[0].text
        # usage ì•ˆì „ ì¶”ì¶œ (SDK ë²„ì „ë³„ ì†ì„±/ë”•íŠ¸ ì°¨ì´ ëŒ€ì‘)
        input_tokens = 0
        output_tokens = 0
        try:
            usage_obj = getattr(message, "usage", None)
            if usage_obj is not None:
                # ê°ì²´ ì†ì„± ìŠ¤íƒ€ì¼
                if hasattr(usage_obj, "input_tokens"):
                    input_tokens = int(getattr(usage_obj, "input_tokens") or 0)
                if hasattr(usage_obj, "output_tokens"):
                    output_tokens = int(getattr(usage_obj, "output_tokens") or 0)
                # ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼
                if isinstance(usage_obj, dict):
                    input_tokens = int(usage_obj.get("input_tokens") or input_tokens or 0)
                    output_tokens = int(usage_obj.get("output_tokens") or output_tokens or 0)
        except Exception:
            # usage íŒŒì‹± ì‹¤íŒ¨ ì‹œ 0ìœ¼ë¡œ ì²˜ë¦¬
            input_tokens = input_tokens or 0
            output_tokens = output_tokens or 0

        return {
            "text": result_text,
            "usage": {
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": (input_tokens or 0) + (output_tokens or 0),
            },
            "model": model_name,
        }

    except ImportError:
        print("[ERROR] anthropic not installed: pip install anthropic")
        return None
    except Exception as e:
        print(f"[ERROR] Translation failed: {e}")
        return None


def translate_chunks(
    chunks: List[dict],
    source_lang: str = "English",
    target_lang: str = "Korean",
    api_key: Optional[str] = None,
    max_workers: int = 20,
    glossary: Optional[dict] = None
) -> List[str]:
    """
    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•œ ëª¨ë“  ì²­í¬ì˜ íš¨ìœ¨ì  ë²ˆì—­

    ì´ í•¨ìˆ˜ëŠ” ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì²­í¬ë¥¼ ë™ì‹œì— ë²ˆì—­í•©ë‹ˆë‹¤:

    1. ë³‘ë ¬ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜:
       - ThreadPoolExecutor(max_workers=5)ë¡œ 5ê°œ ìŠ¤ë ˆë“œ ë™ì‹œ ì‹¤í–‰
       - as_completed()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì™„ë£Œëœ ì‘ì—…ë¶€í„° ì²˜ë¦¬
       - ìˆœì°¨ ì²˜ë¦¬ ëŒ€ë¹„ 5-6ë°° ì„±ëŠ¥ í–¥ìƒ

    2. ì„±ëŠ¥ ìµœì í™”:
       - ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´): 11ê°œ ì²­í¬ Ã— 24.9ì´ˆ = 273.9ì´ˆ (4.5ë¶„)
       - ë³‘ë ¬ ì²˜ë¦¬ (í˜„ì¬): 11ê°œ ì²­í¬ Ã· 5 workers = 45-50ì´ˆ (1ë¶„)
       - Speedup: 5-6ë°° í–¥ìƒ

    3. ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ:
       - ê° ì²­í¬ ì™„ë£Œ ì‹œ ì¦‰ì‹œ ì§„í–‰ ìƒí™© ì¶œë ¥
       - í¬ë§·: [ì™„ë£Œ/ì „ì²´] Chunk N ì™„ë£Œ (í¬ê¸°, ì†Œìš”ì‹œê°„)
       - ë‚¨ì€ ì‘ì—… ìˆ˜ í‘œì‹œ

    4. ì—ëŸ¬ ì²˜ë¦¬:
       - ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
       - ë¶€ë¶„ ì‹¤íŒ¨í•´ë„ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê³„ì† ì§„í–‰

    5. ìµœì¢… í†µê³„:
       - ì´ ì†Œìš”ì‹œê°„, ì²­í¬ë‹¹ í‰ê· ì‹œê°„
       - ë³‘ë ¬ë„ (ì›Œì»¤ ê°œìˆ˜)
       - ì ìš©ëœ ê°€ì´ë“œë¼ì¸

    ì›Œì»¤ ê°œìˆ˜ ê¶Œì¥ì‚¬í•­:
    - max_workers=3: ì•ˆì •ì  (API ë ˆì´íŠ¸ í•œê³„ ê³ ë ¤)
    - max_workers=5: ë¹ ë¥¸ ì²˜ë¦¬ (ê¸°ë³¸ê°’, ê¶Œì¥)
    - max_workers>5: API ì˜¤ë¥˜ ìœ„í—˜ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)

    ì»¨í…ìŠ¤íŠ¸ ì¸ì‹:
    - ê° ì²­í¬ëŠ” ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ê³¼ í•¨ê»˜ ì „ë‹¬ë¨
    - ë²ˆì—­ ì¼ê´€ì„± ë³´ì¥
    - ì²­í¬ ê²½ê³„ì˜ ì–´ìƒ‰í•¨ ì œê±°

    ì„±ëŠ¥ ì§€í‘œ ì˜ˆì‹œ:
    ```
    [ì™„ë£Œ] 11ê°œ ì²­í¬ ë²ˆì—­ ì™„ë£Œ!
      â€¢ ì†Œìš”ì‹œê°„: 47.3ì´ˆ
      â€¢ í‰ê· ì‹œê°„: 4.3ì´ˆ/ì²­í¬
      â€¢ ë³‘ë ¬ë„: 5ê°œ ì›Œì»¤
      â€¢ ì ìš©ê·œì¹™: TRANSLATION_GUIDELINE.md
    ```

    Args:
        chunks (List[dict]): chunk_text()ì—ì„œ ë°˜í™˜í•œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        source_lang (str): ì›ë¬¸ ì–¸ì–´ (ê¸°ë³¸ "English")
        target_lang (str): ëª©í‘œ ì–¸ì–´ (ê¸°ë³¸ "Korean")
        api_key (Optional[str]): Anthropic API í‚¤
        max_workers (int): ë™ì‹œ ì‹¤í–‰ ì›Œì»¤ ê°œìˆ˜ (ê¸°ë³¸ 5)

    Returns:
        List[str]: ë²ˆì—­ëœ ì²­í¬ë“¤ì„ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•œ ë¦¬ìŠ¤íŠ¸
    """
    start_time = time.time()
    
    print(f"[TRANSLATING] {len(chunks)} chunks (with context-aware translation)...")
    print(f"[PARALLEL] Using {max_workers} workers for faster processing")
    print(f"[STATUS] Starting translation...\n")

    # ê²°ê³¼ë¥¼ ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ì €ì¥í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
    results = {}
    completed_count = 0
    # í† í° ì‚¬ìš©ëŸ‰ ì§‘ê³„: ëª¨ë¸ë³„ input/output/requests
    usage_by_model = defaultdict(lambda: {"input_tokens": 0, "output_tokens": 0, "requests": 0})

    def translate_chunk_wrapper(chunk_info):
        """ê° ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë  ë²ˆì—­ í•¨ìˆ˜"""
        i, chunk_data = chunk_info
        chunk_start = time.time()
        
        # chunk_dataëŠ” ë”•ì…”ë„ˆë¦¬: {'text': '...', 'overlap': '...'}
        chunk_text = chunk_data['text'] if isinstance(chunk_data, dict) else chunk_data
        context = chunk_data.get('overlap') if isinstance(chunk_data, dict) else None
        
        translated = translate_with_claude(
            chunk_text,
            source_lang,
            target_lang,
            api_key,
            chunk_num=i,
            total_chunks=len(chunks),
            context=context,
            glossary=glossary
        )
        
        elapsed = time.time() - chunk_start
        return (i, translated, elapsed)

    # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(translate_chunk_wrapper, (i, chunk)): i 
            for i, chunk in enumerate(chunks, 1)
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë¶€í„° ì²˜ë¦¬
        for future in as_completed(futures):
            i, translated, elapsed = future.result()
            completed_count += 1
            pending_count = len(futures) - completed_count
            
            # translated: None | str | dict
            text_out = None
            model_name = None
            if isinstance(translated, dict):
                text_out = translated.get("text")
                usage = translated.get("usage") or {}
                model_name = translated.get("model")
                # ì§‘ê³„
                if model_name:
                    usage_by_model[model_name]["input_tokens"] += int(usage.get("input_tokens") or 0)
                    usage_by_model[model_name]["output_tokens"] += int(usage.get("output_tokens") or 0)
                    usage_by_model[model_name]["requests"] += 1
            else:
                text_out = translated

            if text_out:
                results[i] = text_out
                print(f"âœ“ [{completed_count:2d}/{len(chunks)}] Chunk {i:2d} ì™„ë£Œ ({len(text_out):5d} chars, {elapsed:5.1f}s) | ë‚¨ì€ì‘ì—…: {pending_count:2d}", flush=True)
            else:
                # ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                original_text = chunks[i-1]['text'] if isinstance(chunks[i-1], dict) else chunks[i-1]
                results[i] = original_text
                print(f"âœ— [{completed_count:2d}/{len(chunks)}] Chunk {i:2d} SKIP (ì›ë³¸ ì‚¬ìš©) | ë‚¨ì€ì‘ì—…: {pending_count:2d}", flush=True)

    # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    translated_chunks = [results[i] for i in range(1, len(chunks) + 1)]

    elapsed = time.time() - start_time
    print()
    print(f"{'='*70}")
    print(f"[ì™„ë£Œ] {len(chunks)}ê°œ ì²­í¬ ë²ˆì—­ ì™„ë£Œ!")
    print(f"  â€¢ ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"  â€¢ í‰ê· ì‹œê°„: {elapsed/len(chunks):.1f}ì´ˆ/ì²­í¬")
    print(f"  â€¢ ë³‘ë ¬ë„: {max_workers}ê°œ ì›Œì»¤")
    print(f"  â€¢ ì ìš©ê·œì¹™: TRANSLATION_GUIDELINE.md")
    # í† í°/ë¹„ìš© ìš”ì•½ (ê³µì‹ ê°€ê²© ê¸°ì¤€)
    if usage_by_model:
        print(f"  â€¢ í† í° ì‚¬ìš©ëŸ‰ ë° ì˜ˆìƒ ë¹„ìš© (Anthropic ê³µì‹ ê°€ê²© ê¸°ì¤€):")
        grand_input = 0
        grand_output = 0
        grand_cost = 0.0
        for model, agg in usage_by_model.items():
            inp = agg["input_tokens"]
            outp = agg["output_tokens"]
            reqs = agg["requests"]
            grand_input += inp
            grand_output += outp
            price = _get_model_pricing(model)
            cost = (inp / 1_000_000.0) * float(price.get("input", 0) or 0) + (outp / 1_000_000.0) * float(price.get("output", 0) or 0)
            grand_cost += cost
            if (price.get("input", 0) or 0) == 0 and (price.get("output", 0) or 0) == 0:
                print(f"    - {model}: input={inp:,} tok, output={outp:,} tok, requests={reqs}")
                print(f"      âš ï¸ ê°€ê²©í‘œ ë¯¸ë“±ë¡ (í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”)")
            else:
                print(f"    - {model} ({reqs}íšŒ í˜¸ì¶œ)")
                print(f"      Input:  {inp:>10,} tokens Ã— ${price['input']:.2f}/M = ${(inp/1_000_000)*price['input']:.4f}")
                print(f"      Output: {outp:>10,} tokens Ã— ${price['output']:.2f}/M = ${(outp/1_000_000)*price['output']:.4f}")
                print(f"      ì†Œê³„: ${cost:.4f}")
        print()
        print(f"    ğŸ’° ì´ ì˜ˆìƒ ë¹„ìš©: ${grand_cost:.4f} USD")
        print(f"       (Input: {grand_input:,} tok | Output: {grand_output:,} tok)")
    print(f"{'='*70}")
    print()

    return translated_chunks


def generate_markdown(
    pdf_name: str,
    translated_chunks: List[str],
    original_text: str,
    pages: int
) -> str:
    """Generate markdown from translated chunks"""
    print(f"[MARKDOWN] Generating markdown document...", flush=True)
    markdown = f"""# {pdf_name} - Korean Translation

**Source**: English PDF
**Target**: Korean (í•œêµ­ì–´)
**Pages**: {pages}
**Characters**: {len(original_text):,}
**Chunks**: {len(translated_chunks)}
**Timestamp**: {time.strftime('%Y-%m-%d %H:%M:%S')}

---

## Content

"""

    for i, translated in enumerate(translated_chunks, 1):
        markdown += f"## Section {i}\n\n"
        markdown += translated + "\n\n"
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ (ë§¤ 5ì„¹ì…˜ë§ˆë‹¤)
        if i % 5 == 0 or i == len(translated_chunks):
            progress = (i / len(translated_chunks)) * 100
            print(f"  [{i:3d}/{len(translated_chunks)}] {progress:5.1f}% complete", flush=True)

    markdown += f"---\n\n"
    markdown += f"**Translation completed**: All {len(translated_chunks)} sections translated successfully."

    print(f"[OK] Markdown document generated", flush=True)
    return markdown


def main():
    print("=" * 70)
    print("[COMPLETE PDF TRANSLATION PIPELINE]")
    print("=" * 70)
    print()

    # Check API key
    api_key = get_api_key()
    if not api_key:
        print("[ERROR] ANTHROPIC_API_KEY not set")
        print()
        print("Setup instructions in: CLAUDE_API_SETUP.md")
        print()
        print("Quick setup:")
        print("  1. Get API key: https://console.anthropic.com")
        print("  2. Set environment: export ANTHROPIC_API_KEY=sk-ant-...")
        print("  3. Run again: python translate_full_pdf.py")
        return

    print("[OK] API key configured")
    print()

    # Get PDF path from CLI argument or use default
    if len(sys.argv) > 1:
        pdf_path = Path(sys.argv[1])
        if not pdf_path.is_absolute():
            # ìƒëŒ€ ê²½ë¡œë©´ input/ í´ë” ê¸°ì¤€ìœ¼ë¡œ
            pdf_path = Path("input") / pdf_path
    else:
        # ê¸°ë³¸ê°’: input/laf.pdf
        pdf_path = Path('input/laf.pdf')

    if not pdf_path.exists():
        print(f"[ERROR] PDF not found: {pdf_path}")
        print()
        print("ì‚¬ìš©ë²•:")
        print("  python translate_full_pdf.py laf.pdf")
        print("  python translate_full_pdf.py input/my_book.pdf")
        print("  python translate_full_pdf.py /absolute/path/to/book.pdf")
        return

    print(f"[PDF] {pdf_path.name} ({pdf_path.absolute()})")
    print()

    # output í´ë” ìƒì„±
    Path("output").mkdir(exist_ok=True)
    print()

    # Extract
    print()
    print("[STEP 1/5] Extract PDF")
    print("-" * 70)
    text, metadata, pages = extract_pdf(pdf_path)
    if not text:
        print("[ERROR] Failed to extract text from PDF")
        return

    print(f"[OK] âœ“ Extracted {len(text):,} characters from {len(pages)} pages")
    print()

    # Glossary extraction
    print("[STEP 2/5] Analyze document & extract glossary")
    print("-" * 70)
    glossary = extract_glossary(text, api_key)
    
    if glossary and glossary.get("key_terms"):
        print(f"[OK] âœ“ Document domain: {glossary.get('domain', 'unknown')}")
        print(f"[OK] âœ“ Extracted {len(glossary['key_terms'])} key terms")
        # ìƒ˜í”Œ í‘œì‹œ
        sample_terms = list(glossary['key_terms'].items())[:5]
        for eng, kor in sample_terms:
            print(f"      â€¢ {eng} â†’ {kor}")
        if len(glossary['key_terms']) > 5:
            print(f"      ... and {len(glossary['key_terms']) - 5} more")
    else:
        print(f"[INFO] No custom glossary - using general translation guidelines")
    print()

    # Chunk
    print("[STEP 3/5] Create chunks")
    print("-" * 70)
    chunks = chunk_text(text, chunk_size=5000)
    print(f"[OK] âœ“ Total chunks to translate: {len(chunks)}")
    print()

    # Translate
    print("[STEP 4/5] Translate with Claude API (ë³‘ë ¬ ì²˜ë¦¬)")
    print("-" * 70)
    translated_chunks = translate_chunks(
        chunks, "English", "Korean", api_key,
        glossary=glossary
    )

    if not translated_chunks:
        print("[ERROR] Translation failed")
        return

    print()
    
    # Generate markdown
    print("[STEP 5/5] Generate markdown")
    print("-" * 70)
    markdown = generate_markdown(pdf_path.stem, translated_chunks, text, len(pages))
    print()

    # output/ í´ë”ì— ì €ì¥
    print("[SAVING] Writing output file...")
    output_path = Path('output') / f'output_{pdf_path.stem}_translated.md'
    output_path.write_text(markdown, encoding='utf-8')
    print(f"[OK] âœ“ Output saved: {output_path.name}")
    print()

    # Summary
    print("=" * 70)
    print("[âœ“ SUMMARY]")
    print("=" * 70)
    print(f"  ğŸ“„ PDF File: {pdf_path.name}")
    print(f"  ğŸ“– Pages: {len(pages)}")
    print(f"  ğŸ“ Total Characters: {len(text):,}")
    print(f"  ğŸ“¦ Chunks Created: {len(chunks)}")
    print(f"  ğŸŒ Chunks Translated: {len(translated_chunks)}")
    print(f"  ğŸ’¾ Output File: {output_path.name}")
    print(f"  ğŸ“ Location: {output_path.absolute()}")
    print("=" * 70)
    print()
    print("[SUCCESS] âœ¨ Complete translation pipeline finished!")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"[ERROR] {e}")
        import traceback
        traceback.print_exc()
