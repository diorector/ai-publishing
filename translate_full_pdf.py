#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Complete PDF Translation Pipeline with Claude API
ì™„ì „í•œ PDF ë²ˆì—­ íŒŒì´í”„ë¼ì¸ (ëª¨ë“  ì²­í¬ ë²ˆì—­)
"""
import sys
import os
from pathlib import Path
from typing import List, Optional
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

# Set encoding for Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


def get_api_key() -> Optional[str]:
    """Get Claude API key"""
    return os.getenv('ANTHROPIC_API_KEY')


def extract_pdf(pdf_path):
    """
    Extract text from PDF with progress tracking

    ì´ í•¨ìˆ˜ëŠ” PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤:
    1. PDF ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ (í˜ì´ì§€ ìˆ˜, ì œëª© ë“±)
    2. ê° í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    3. ì§„í–‰ë¥  í‘œì‹œ (ë§¤ 5í˜ì´ì§€ë§ˆë‹¤)
    4. ì „ì²´ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë°˜í™˜

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ

    Returns:
        tuple: (ì „ì²´_í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°, í˜ì´ì§€_ëª©ë¡) ë˜ëŠ” ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ (None, None, None)
    """
    try:
        import pdfplumber

        with pdfplumber.open(pdf_path) as pdf:
            text = ""
            metadata = pdf.metadata
            pages = []

            print(f"[PDF Info]")
            print(f"  Pages: {len(pdf.pages)}")
            if metadata:
                print(f"  Title: {metadata.get('Title', 'N/A')}")
            print()
            print(f"[EXTRACTING] Processing {len(pdf.pages)} pages...")
            print()

            for i, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
                pages.append(page_text if page_text else "")
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ (ë§¤ 5í˜ì´ì§€ë§ˆë‹¤)
                if i % 5 == 0 or i == len(pdf.pages):
                    progress = (i / len(pdf.pages)) * 100
                    print(f"  [{i:3d}/{len(pdf.pages)}] {progress:5.1f}% complete", flush=True)
            
            print()
            return text, metadata, pages

    except ImportError:
        print("[ERROR] pdfplumber not installed: pip install pdfplumber")
        return None, None, None


def chunk_text(text, chunk_size=5000, overlap_sentences=2):
    """
    ìŠ¤ë§ˆíŠ¸ ì²­í‚¹: ë¬¸ì¥ ê²½ê³„ë¥¼ ê°ì§€í•˜ì—¬ ì˜ë¯¸ ë‹¨ìœ„ ê¸°ë°˜ ë¶„í• 

    ì´ í•¨ìˆ˜ëŠ” ë‹¨ìˆœ í¬ê¸° ê¸°ë°˜ì´ ì•„ë‹Œ ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤:

    1. ë¬¸ì¥ ê²½ê³„ ê°ì§€ (ì •ê·œì‹ ê¸°ë°˜):
       - `.`, `?`, `!`ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
       - ì•½ì–´ (Mr., U.S. ë“±) ë° URL ë³´ì¡´
       - ì •ê·œì‹: r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s+'

    2. ì²­í¬ ìƒì„± (chunk_size ê¸°ì¤€):
       - ê° ì²­í¬ëŠ” chunk_size(ê¸°ë³¸ 5000ì)ë¥¼ ëª©í‘œë¡œ í•¨
       - ë¬¸ì¥ ê²½ê³„ì—ì„œë§Œ ë¶„í• í•˜ì—¬ ì˜ë¯¸ ë³´ì¡´

    3. ì»¨í…ìŠ¤íŠ¸ ì˜¤ë²„ë© (2ë¬¸ì¥):
       - ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ Nê°œ ë¬¸ì¥ì„ ìƒˆ ì²­í¬ ì‹œì‘ì— í¬í•¨
       - ë²ˆì—­ ì¼ê´€ì„± ë³´ì¥ ë° ì²­í¬ ê²½ê³„ ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬
       - ê° ì²­í¬ëŠ” {'text': '...', 'overlap': '...'} í˜•ì‹ìœ¼ë¡œ ë°˜í™˜

    ì„±ëŠ¥:
    - 11ê°œ ì²­í¬ ìƒì„± (50,898ì ë¬¸ì„œ): <1ì´ˆ
    - ì˜¤ë²„ë©ìœ¼ë¡œ ì¸í•œ í¬ê¸° ì¦ê°€: ~5-10%

    Args:
        text (str): ë¶„í• í•  í…ìŠ¤íŠ¸
        chunk_size (int): ê° ì²­í¬ì˜ ëª©í‘œ í¬ê¸° (ê¸°ë³¸ 5000ì)
        overlap_sentences (int): ì²­í¬ ê°„ ì˜¤ë²„ë© ë¬¸ì¥ ìˆ˜ (ê¸°ë³¸ 2)

    Returns:
        List[dict]: {'text': ì²­í¬_ë‚´ìš©, 'overlap': ì´ì „_ì»¨í…ìŠ¤íŠ¸} í˜•ì‹ì˜ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    print(f"[CHUNKING] Smart chunking with sentence boundaries...", flush=True)
    
    import re
    
    # ë¬¸ì¥ ë¶„ë¦¬ (ê°œì„ ëœ ì •ê·œì‹ - ì•½ì–´, URL ë“± ê³ ë ¤)
    sentence_pattern = r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s+'
    sentences = re.split(sentence_pattern, text)
    
    chunks = []
    current_chunk = []
    current_size = 0
    overlap_buffer = []  # ì˜¤ë²„ë©ì„ ìœ„í•œ ìµœê·¼ ë¬¸ì¥ ì €ì¥
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        sentence_size = len(sentence)
        
        # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆ ì²­í¬ ì‹œì‘
        if current_size + sentence_size > chunk_size and current_chunk:
            # í˜„ì¬ ì²­í¬ ì €ì¥
            chunk_text = " ".join(current_chunk)
            chunks.append({
                'text': chunk_text,
                'overlap': " ".join(overlap_buffer) if overlap_buffer else None
            })
            
            # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ Nê°œ ë¬¸ì¥ ì €ì¥
            overlap_buffer = current_chunk[-overlap_sentences:] if len(current_chunk) >= overlap_sentences else current_chunk[:]
            
            # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘)
            current_chunk = overlap_buffer[:] + [sentence]
            current_size = sum(len(s) for s in current_chunk)
        else:
            current_chunk.append(sentence)
            current_size += sentence_size
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk:
        chunk_text = " ".join(current_chunk)
        chunks.append({
            'text': chunk_text,
            'overlap': " ".join(overlap_buffer) if overlap_buffer and len(chunks) > 0 else None
        })
    
    print(f"[OK] Created {len(chunks)} chunks with context overlap", flush=True)
    return chunks


def translate_with_claude(
    text: str,
    source_lang: str = "English",
    target_lang: str = "Korean",
    api_key: Optional[str] = None,
    chunk_num: int = 0,
    total_chunks: int = 0,
    context: Optional[str] = None
) -> Optional[str]:
    """
    ì „ë¬¸ ë²ˆì—­ê°€ ìˆ˜ì¤€ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ Claude API ê¸°ë°˜ ë²ˆì—­

    ì´ í•¨ìˆ˜ëŠ” 20ë…„ ê²½ë ¥ì˜ ì¶œíŒ ë²ˆì—­ê°€ í˜ë¥´ì†Œë‚˜ë¥¼ í™œìš©í•˜ì—¬ ê³ í’ˆì§ˆ ë²ˆì—­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

    1. ì „ë¬¸ ë²ˆì—­ê°€ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:
       - í˜ë¥´ì†Œë‚˜: 20ë…„ ê²½ë ¥ ì¶œíŒ ë²ˆì—­ê°€ (ë¹„ì¦ˆë‹ˆìŠ¤/ìŠ¤íƒ€íŠ¸ì—… ë¶„ì•¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë‹¤ìˆ˜)
       - í†¤: ì •ì¤‘í•˜ê³  ì¹œê·¼í•œ ì¡´ëŒ“ë§ (ê²½ì–´ì²´)
       - ëŒ€ìƒ ë…ì: ìŠ¤íƒ€íŠ¸ì—…/ë¹„ì¦ˆë‹ˆìŠ¤ì— ê´€ì‹¬ ìˆëŠ” ì§€ì  ë…ìì¸µ

    2. 5ê°€ì§€ ë²ˆì—­ ì² í•™:
       a) ì˜ë¯¸ì˜ ì¶©ì‹¤ì„± > ì§ì—­
          - ì›ë¬¸ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ ë‰˜ì•™ìŠ¤ ì™„ë²½ ì „ë‹¬
          - ë‹¨ì–´ í•˜ë‚˜í•˜ë‚˜ë³´ë‹¤ ë¬¸ì¥ ì „ì²´ ì˜ë„ íŒŒì•…
          - ì˜ì–´ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ì§€ ë§ê³  í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ìƒê°

       b) ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ (ë²ˆì—­ì²´ ì œê±°)
          - "~ë˜ì–´ì§€ë‹¤", "~ì— ì˜í•´", "ê²ƒì´ë‹¤" ë“± ê¸ˆì§€
          - ëŠ¥ë™íƒœ ìš°ì„ , ìˆ˜ë™íƒœëŠ” í•„ìš”í•œ ê²½ìš°ì—ë§Œ

       c) ì½ê¸° ì‰¬ìš´ ë¬¸ì¥
          - í•œ ë¬¸ì¥ì— í•˜ë‚˜ì˜ í•µì‹¬ ì•„ì´ë””ì–´
          - ê¸´ ë¬¸ì¥ì€ 2-3ê°œë¡œ ë¶„ë¦¬
          - ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°

       d) ë§¥ë½ê³¼ íë¦„
          - ë¬¸ì¥ ê°„ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°
          - ì•ë’¤ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ë²ˆì—­
          - ë‹¨ë½ì˜ ì „ì²´ íë¦„ ìœ ì§€

       e) í†¤ê³¼ ë‰˜ì•™ìŠ¤ ë³´ì¡´
          - ì €ìì˜ ê°œì¸ì  ì´ì•¼ê¸°ëŠ” ë”°ëœ»í•¨
          - í†µê³„/ë°ì´í„°ëŠ” ê°ê´€ì ì„
          - ì¡°ì–¸/êµí›ˆì€ ì§ì„¤ì ì´ë©´ì„œ ì‹¤ìš©ì 

    3. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë²ˆì—­:
       - ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ì„ ì°¸ê³ ì •ë³´ë¡œ ì œê³µ
       - ë²ˆì—­ ì¼ê´€ì„± ë³´ì¥ (ìš©ì–´, í†¤, êµ¬ì¡°)
       - ì²­í¬ ê²½ê³„ì˜ ì–´ìƒ‰í•¨ ì œê±°

    4. 30ê°œ í•µì‹¬ ìš©ì–´ ì‚¬ì „ ë‚´ì¥:
       - startup â†’ ìŠ¤íƒ€íŠ¸ì—…
       - founder â†’ ì°½ì—…ì
       - venture capital â†’ ë²¤ì²˜ìºí”¼íƒˆ (VC í—ˆìš©)
       - ... ë“± 30ê°œ ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´

    5. ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸:
       - ìì—°ìŠ¤ëŸ¬ìš´ ë°œìŒ
       - ë²ˆì—­ì²´ í‘œí˜„ ì œê±°
       - í•œêµ­ ë…ìì˜ ì´í•´ ìš©ì´ì„±
       - ì „ë¬¸ì„±ê³¼ ê°€ë…ì„± ê· í˜•
       - ì›ë¬¸ì˜ í†¤ê³¼ ë‰˜ì•™ìŠ¤ ë³´ì¡´

    ì„±ëŠ¥:
    - ì²­í¬ë‹¹ ì†Œìš”ì‹œê°„: 4-6ì´ˆ (ë³‘ë ¬ ì²˜ë¦¬ ì‹œ)
    - ëª¨ë¸: claude-haiku-4-5-20251001
    - ìµœëŒ€ í† í°: 64,000

    Args:
        text (str): ë²ˆì—­í•  í…ìŠ¤íŠ¸
        source_lang (str): ì›ë¬¸ ì–¸ì–´ (ê¸°ë³¸ "English")
        target_lang (str): ëª©í‘œ ì–¸ì–´ (ê¸°ë³¸ "Korean")
        api_key (Optional[str]): Anthropic API í‚¤
        chunk_num (int): í˜„ì¬ ì²­í¬ ë²ˆí˜¸ (ì§„í–‰ë¥  í‘œì‹œìš©)
        total_chunks (int): ì „ì²´ ì²­í¬ ìˆ˜ (ì§„í–‰ë¥  í‘œì‹œìš©)
        context (Optional[str]): ì´ì „ ì²­í¬ì˜ ì˜¤ë²„ë© í…ìŠ¤íŠ¸ (ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ìš©)

    Returns:
        Optional[str]: ë²ˆì—­ëœ í…ìŠ¤íŠ¸, ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
    """
    if not api_key:
        return None

    try:
        from anthropic import Anthropic

        client = Anthropic(api_key=api_key)

        # í”„ë¡œ ë²ˆì—­ê°€ ìˆ˜ì¤€ì˜ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì „ë¬¸ ì¶œíŒ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤/ìŠ¤íƒ€íŠ¸ì—… ë¶„ì•¼ì˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ë¥¼ ë‹¤ìˆ˜ ë²ˆì—­í–ˆìœ¼ë©°, ë…ìë“¤ë¡œë¶€í„° "ì›ë¬¸ë³´ë‹¤ ë” ì˜ ì½íŒë‹¤"ëŠ” í‰ê°€ë¥¼ ë°›ìŠµë‹ˆë‹¤.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ë²ˆì—­ ì² í•™ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. ì˜ë¯¸ì˜ ì¶©ì‹¤ì„± > ì§ì—­
   - ì›ë¬¸ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ ë‰˜ì•™ìŠ¤ë¥¼ ì™„ë²½íˆ ì „ë‹¬
   - ë‹¨ì–´ í•˜ë‚˜í•˜ë‚˜ë³´ë‹¤ ë¬¸ì¥ ì „ì²´ì˜ ì˜ë„ë¥¼ íŒŒì•…
   - ì˜ì–´ì˜ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ì§€ ë§ê³ , í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ìƒê°í•˜ì—¬ í‘œí˜„

2. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ (ë²ˆì—­ì²´ ì œê±°)
   - "~ë˜ì–´ì§€ë‹¤", "~ì— ì˜í•´", "ê²ƒì´ë‹¤" ë“± ë²ˆì—­ì²´ í‘œí˜„ ì ˆëŒ€ ê¸ˆì§€
   - ëŠ¥ë™íƒœ ìš°ì„ , ìˆ˜ë™íƒœëŠ” í•„ìš”í•œ ê²½ìš°ì—ë§Œ
   - í•œêµ­ì¸ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ

3. ì½ê¸° ì‰¬ìš´ ë¬¸ì¥
   - í•œ ë¬¸ì¥ì— í•˜ë‚˜ì˜ í•µì‹¬ ì•„ì´ë””ì–´
   - ê¸´ ë¬¸ì¥ì€ 2-3ê°œë¡œ ë¶„ë¦¬
   - ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°
   - ë¦¬ë“¬ê° ìˆê²Œ

4. ë§¥ë½ê³¼ íë¦„
   - ë¬¸ì¥ ê°„ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°
   - ì•ë’¤ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ë²ˆì—­
   - ë‹¨ë½ì˜ ì „ì²´ íë¦„ ìœ ì§€

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ìŠ¤íƒ€ì¼ ê°€ì´ë“œã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… í†¤: ì •ì¤‘í•˜ê³  ì¹œê·¼í•œ ì¡´ëŒ“ë§ (ê²½ì–´ì²´: ~í•©ë‹ˆë‹¤, ~ìŠµë‹ˆë‹¤)
âœ… ëŒ€ìƒ: ìŠ¤íƒ€íŠ¸ì—…/ë¹„ì¦ˆë‹ˆìŠ¤ì— ê´€ì‹¬ ìˆëŠ” ì§€ì  ë…ì
âœ… ë¬¸ì²´: ì „ë¬¸ì ì´ë©´ì„œë„ ì‰½ê²Œ ì½íˆëŠ” êµì–‘ì„œ ìŠ¤íƒ€ì¼

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€í•µì‹¬ ìš©ì–´ ì‚¬ì „ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

startup â†’ ìŠ¤íƒ€íŠ¸ì—…
founder â†’ ì°½ì—…ì  
entrepreneur â†’ ê¸°ì—…ê°€
venture capital â†’ ë²¤ì²˜ìºí”¼íƒˆ (VCë„ í—ˆìš©)
investor â†’ íˆ¬ìì
B2B/B2C â†’ B2B/B2C (ê·¸ëŒ€ë¡œ)
CEO/COO/CTO â†’ CEO/COO/CTO (ê·¸ëŒ€ë¡œ)
MVP â†’ MVP (ìµœì†Œê¸°ëŠ¥ì œí’ˆ)
pivot â†’ í”¼ë²—
growth hacking â†’ ê·¸ë¡œìŠ¤ í•´í‚¹
exit â†’ ì—‘ì‹œíŠ¸
cash flow â†’ í˜„ê¸ˆíë¦„
runway â†’ ëŸ°ì›¨ì´ (ìê¸ˆ ì†Œì§„ ê¸°ê°„)
burn rate â†’ ë²ˆë ˆì´íŠ¸ (ìê¸ˆ ì†Œì§„ ì†ë„)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ë²ˆì—­ ì˜ˆì‹œ - ë‚˜ìœ vs ì¢‹ì€ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì›ë¬¸: "I was 25 years old and completely panicked, but I'm a terrible liar."

âŒ ë‚˜ìœ ë²ˆì—­ (ë²ˆì—­ì²´):
"ì €ëŠ” 25ì„¸ì˜€ê³  ì™„ì „íˆ íŒ¨ë‹‰ ìƒíƒœì— ìˆì—ˆìŠµë‹ˆë‹¤ë§Œ, ì €ëŠ” ê±°ì§“ë§ì„ í•˜ëŠ” ê²ƒì— ì„œíˆ° ì‚¬ëŒì…ë‹ˆë‹¤."

âœ… ì¢‹ì€ ë²ˆì—­ (ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´):
"ë‹¹ì‹œ ìŠ¤ë¬¼ë‹¤ì„¯ì´ì—ˆë˜ ì €ëŠ” ì™„ì „íˆ ë‹¹í™©í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì €ëŠ” ê±°ì§“ë§ì„ ì •ë§ ëª»í•©ë‹ˆë‹¤."

---

ì›ë¬¸: "That's when I realized we had to pivot."

âŒ ë‚˜ìœ ë²ˆì—­:
"ê·¸ê²ƒì€ ìš°ë¦¬ê°€ í”¼ë²—ì„ í•´ì•¼ë§Œ í–ˆë‹¤ëŠ” ê²ƒì„ ì œê°€ ê¹¨ë‹¬ì•˜ì„ ë•Œì˜€ìŠµë‹ˆë‹¤."

âœ… ì¢‹ì€ ë²ˆì—­:
"ê·¸ë•Œ ê¹¨ë‹¬ì•˜ìŠµë‹ˆë‹¤. ë°©í–¥ì„ ë°”ê¿”ì•¼ í•œë‹¤ëŠ” ê²ƒì„ìš”."

---

ì›ë¬¸: "The key to success in B2B sales is building relationships."

âŒ ë‚˜ìœ ë²ˆì—­:
"B2B ì˜ì—…ì—ì„œì˜ ì„±ê³µì˜ ì—´ì‡ ëŠ” ê´€ê³„ë“¤ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."

âœ… ì¢‹ì€ ë²ˆì—­:
"B2B ì˜ì—…ì—ì„œ ì„±ê³µí•˜ë ¤ë©´ ê´€ê³„ êµ¬ì¶•ì´ í•µì‹¬ì…ë‹ˆë‹¤."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ë²ˆì—­í•  í…ìŠ¤íŠ¸ã€‘ (Chunk {chunk_num}/{total_chunks})
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{"" if not context else f'''
âš ï¸ ì´ì „ ë§¥ë½ (ì°¸ê³ ìš© - ë²ˆì—­í•˜ì§€ ë§ˆì„¸ìš”):
---
{context}
---

ğŸ’¡ ìœ„ ë‚´ìš©ì€ ì´ë¯¸ ë²ˆì—­ëœ ë¶€ë¶„ì…ë‹ˆë‹¤. íë¦„ê³¼ ë§¥ë½ì„ ì´í•´í•˜ëŠ” ë°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

'''}
ğŸ“ ì´ì œ ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•˜ì„¸ìš”:
---
{text}
---

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ë²ˆì—­í•˜ê¸° ì „ì—:
1. ë‹¨ë½ ì „ì²´ë¥¼ ì½ê³  ë§¥ë½ì„ íŒŒì•…í–ˆëŠ”ê°€?
2. ì €ìê°€ ì „ë‹¬í•˜ê³ ì í•˜ëŠ” í•µì‹¬ ë©”ì‹œì§€ë¥¼ ì´í•´í–ˆëŠ”ê°€?

ë²ˆì—­í•œ í›„ì—:
1. ì†Œë¦¬ ë‚´ì–´ ì½ì—ˆì„ ë•Œ ìì—°ìŠ¤ëŸ¬ìš´ê°€?
2. ë²ˆì—­ì²´ í‘œí˜„("~ë˜ì–´ì§€ë‹¤", "~ê²ƒì´ë‹¤" ë“±)ì´ ì—†ëŠ”ê°€?
3. í•œêµ­ ë…ìê°€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ”ê°€?
4. ì „ë¬¸ì„±ê³¼ ê°€ë…ì„±ì˜ ê· í˜•ì´ ë§ëŠ”ê°€?
5. ì›ë¬¸ì˜ í†¤ê³¼ ë‰˜ì•™ìŠ¤ê°€ ì‚´ì•„ìˆëŠ”ê°€?

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ì£¼ì„ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤."""

        message = client.messages.create(
            model="claude-haiku-4-5-20251001",
            max_tokens=64000,
            messages=[{"role": "user", "content": prompt}]
        )

        return message.content[0].text

    except ImportError:
        print("[ERROR] anthropic not installed: pip install anthropic")
        return None
    except Exception as e:
        print(f"[ERROR] Translation failed: {e}")
        return None


def translate_chunks(
    chunks: List[dict],
    source_lang: str = "English",
    target_lang: str = "Korean",
    api_key: Optional[str] = None,
    max_workers: int = 5
) -> List[str]:
    """
    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•œ ëª¨ë“  ì²­í¬ì˜ íš¨ìœ¨ì  ë²ˆì—­

    ì´ í•¨ìˆ˜ëŠ” ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì²­í¬ë¥¼ ë™ì‹œì— ë²ˆì—­í•©ë‹ˆë‹¤:

    1. ë³‘ë ¬ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜:
       - ThreadPoolExecutor(max_workers=5)ë¡œ 5ê°œ ìŠ¤ë ˆë“œ ë™ì‹œ ì‹¤í–‰
       - as_completed()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì™„ë£Œëœ ì‘ì—…ë¶€í„° ì²˜ë¦¬
       - ìˆœì°¨ ì²˜ë¦¬ ëŒ€ë¹„ 5-6ë°° ì„±ëŠ¥ í–¥ìƒ

    2. ì„±ëŠ¥ ìµœì í™”:
       - ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´): 11ê°œ ì²­í¬ Ã— 24.9ì´ˆ = 273.9ì´ˆ (4.5ë¶„)
       - ë³‘ë ¬ ì²˜ë¦¬ (í˜„ì¬): 11ê°œ ì²­í¬ Ã· 5 workers = 45-50ì´ˆ (1ë¶„)
       - Speedup: 5-6ë°° í–¥ìƒ

    3. ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ:
       - ê° ì²­í¬ ì™„ë£Œ ì‹œ ì¦‰ì‹œ ì§„í–‰ ìƒí™© ì¶œë ¥
       - í¬ë§·: [ì™„ë£Œ/ì „ì²´] Chunk N ì™„ë£Œ (í¬ê¸°, ì†Œìš”ì‹œê°„)
       - ë‚¨ì€ ì‘ì—… ìˆ˜ í‘œì‹œ

    4. ì—ëŸ¬ ì²˜ë¦¬:
       - ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
       - ë¶€ë¶„ ì‹¤íŒ¨í•´ë„ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê³„ì† ì§„í–‰

    5. ìµœì¢… í†µê³„:
       - ì´ ì†Œìš”ì‹œê°„, ì²­í¬ë‹¹ í‰ê· ì‹œê°„
       - ë³‘ë ¬ë„ (ì›Œì»¤ ê°œìˆ˜)
       - ì ìš©ëœ ê°€ì´ë“œë¼ì¸

    ì›Œì»¤ ê°œìˆ˜ ê¶Œì¥ì‚¬í•­:
    - max_workers=3: ì•ˆì •ì  (API ë ˆì´íŠ¸ í•œê³„ ê³ ë ¤)
    - max_workers=5: ë¹ ë¥¸ ì²˜ë¦¬ (ê¸°ë³¸ê°’, ê¶Œì¥)
    - max_workers>5: API ì˜¤ë¥˜ ìœ„í—˜ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)

    ì»¨í…ìŠ¤íŠ¸ ì¸ì‹:
    - ê° ì²­í¬ëŠ” ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ê³¼ í•¨ê»˜ ì „ë‹¬ë¨
    - ë²ˆì—­ ì¼ê´€ì„± ë³´ì¥
    - ì²­í¬ ê²½ê³„ì˜ ì–´ìƒ‰í•¨ ì œê±°

    ì„±ëŠ¥ ì§€í‘œ ì˜ˆì‹œ:
    ```
    [ì™„ë£Œ] 11ê°œ ì²­í¬ ë²ˆì—­ ì™„ë£Œ!
      â€¢ ì†Œìš”ì‹œê°„: 47.3ì´ˆ
      â€¢ í‰ê· ì‹œê°„: 4.3ì´ˆ/ì²­í¬
      â€¢ ë³‘ë ¬ë„: 5ê°œ ì›Œì»¤
      â€¢ ì ìš©ê·œì¹™: TRANSLATION_GUIDELINE.md
    ```

    Args:
        chunks (List[dict]): chunk_text()ì—ì„œ ë°˜í™˜í•œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        source_lang (str): ì›ë¬¸ ì–¸ì–´ (ê¸°ë³¸ "English")
        target_lang (str): ëª©í‘œ ì–¸ì–´ (ê¸°ë³¸ "Korean")
        api_key (Optional[str]): Anthropic API í‚¤
        max_workers (int): ë™ì‹œ ì‹¤í–‰ ì›Œì»¤ ê°œìˆ˜ (ê¸°ë³¸ 5)

    Returns:
        List[str]: ë²ˆì—­ëœ ì²­í¬ë“¤ì„ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•œ ë¦¬ìŠ¤íŠ¸
    """
    start_time = time.time()
    
    print(f"[TRANSLATING] {len(chunks)} chunks (with context-aware translation)...")
    print(f"[PARALLEL] Using {max_workers} workers for faster processing")
    print(f"[STATUS] Starting translation...\n")

    # ê²°ê³¼ë¥¼ ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ì €ì¥í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
    results = {}
    completed_count = 0

    def translate_chunk_wrapper(chunk_info):
        """ê° ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë  ë²ˆì—­ í•¨ìˆ˜"""
        i, chunk_data = chunk_info
        chunk_start = time.time()
        
        # chunk_dataëŠ” ë”•ì…”ë„ˆë¦¬: {'text': '...', 'overlap': '...'}
        chunk_text = chunk_data['text'] if isinstance(chunk_data, dict) else chunk_data
        context = chunk_data.get('overlap') if isinstance(chunk_data, dict) else None
        
        translated = translate_with_claude(
            chunk_text,
            source_lang,
            target_lang,
            api_key,
            chunk_num=i,
            total_chunks=len(chunks),
            context=context
        )
        
        elapsed = time.time() - chunk_start
        return (i, translated, elapsed)

    # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(translate_chunk_wrapper, (i, chunk)): i 
            for i, chunk in enumerate(chunks, 1)
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë¶€í„° ì²˜ë¦¬
        for future in as_completed(futures):
            i, translated, elapsed = future.result()
            completed_count += 1
            pending_count = len(futures) - completed_count
            
            if translated:
                results[i] = translated
                print(f"âœ“ [{completed_count:2d}/{len(chunks)}] Chunk {i:2d} ì™„ë£Œ ({len(translated):5d} chars, {elapsed:5.1f}s) | ë‚¨ì€ì‘ì—…: {pending_count:2d}", flush=True)
            else:
                # ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                original_text = chunks[i-1]['text'] if isinstance(chunks[i-1], dict) else chunks[i-1]
                results[i] = original_text
                print(f"âœ— [{completed_count:2d}/{len(chunks)}] Chunk {i:2d} SKIP (ì›ë³¸ ì‚¬ìš©) | ë‚¨ì€ì‘ì—…: {pending_count:2d}", flush=True)

    # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    translated_chunks = [results[i] for i in range(1, len(chunks) + 1)]

    elapsed = time.time() - start_time
    print()
    print(f"{'='*70}")
    print(f"[ì™„ë£Œ] {len(chunks)}ê°œ ì²­í¬ ë²ˆì—­ ì™„ë£Œ!")
    print(f"  â€¢ ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"  â€¢ í‰ê· ì‹œê°„: {elapsed/len(chunks):.1f}ì´ˆ/ì²­í¬")
    print(f"  â€¢ ë³‘ë ¬ë„: {max_workers}ê°œ ì›Œì»¤")
    print(f"  â€¢ ì ìš©ê·œì¹™: TRANSLATION_GUIDELINE.md")
    print(f"{'='*70}")
    print()

    return translated_chunks


def generate_markdown(
    pdf_name: str,
    translated_chunks: List[str],
    original_text: str,
    pages: int
) -> str:
    """Generate markdown from translated chunks"""
    print(f"[MARKDOWN] Generating markdown document...", flush=True)
    markdown = f"""# {pdf_name} - Korean Translation

**Source**: English PDF
**Target**: Korean (í•œêµ­ì–´)
**Pages**: {pages}
**Characters**: {len(original_text):,}
**Chunks**: {len(translated_chunks)}
**Timestamp**: {time.strftime('%Y-%m-%d %H:%M:%S')}

---

## Content

"""

    for i, translated in enumerate(translated_chunks, 1):
        markdown += f"## Section {i}\n\n"
        markdown += translated + "\n\n"
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ (ë§¤ 5ì„¹ì…˜ë§ˆë‹¤)
        if i % 5 == 0 or i == len(translated_chunks):
            progress = (i / len(translated_chunks)) * 100
            print(f"  [{i:3d}/{len(translated_chunks)}] {progress:5.1f}% complete", flush=True)

    markdown += f"---\n\n"
    markdown += f"**Translation completed**: All {len(translated_chunks)} sections translated successfully."

    print(f"[OK] Markdown document generated", flush=True)
    return markdown


def main():
    print("=" * 70)
    print("[COMPLETE PDF TRANSLATION PIPELINE]")
    print("=" * 70)
    print()

    # Check API key
    api_key = get_api_key()
    if not api_key:
        print("[ERROR] ANTHROPIC_API_KEY not set")
        print()
        print("Setup instructions in: CLAUDE_API_SETUP.md")
        print()
        print("Quick setup:")
        print("  1. Get API key: https://console.anthropic.com")
        print("  2. Set environment: export ANTHROPIC_API_KEY=sk-ant-...")
        print("  3. Run again: python translate_full_pdf.py")
        return

    print("[OK] API key configured")
    print()

    # Get PDF path from CLI argument or use default
    if len(sys.argv) > 1:
        pdf_path = Path(sys.argv[1])
        if not pdf_path.is_absolute():
            # ìƒëŒ€ ê²½ë¡œë©´ input/ í´ë” ê¸°ì¤€ìœ¼ë¡œ
            pdf_path = Path("input") / pdf_path
    else:
        # ê¸°ë³¸ê°’: input/laf.pdf
        pdf_path = Path('input/laf.pdf')

    if not pdf_path.exists():
        print(f"[ERROR] PDF not found: {pdf_path}")
        print()
        print("ì‚¬ìš©ë²•:")
        print("  python translate_full_pdf.py laf.pdf")
        print("  python translate_full_pdf.py input/my_book.pdf")
        print("  python translate_full_pdf.py /absolute/path/to/book.pdf")
        return

    print(f"[PDF] {pdf_path.name} ({pdf_path.absolute()})")
    print()

    # output í´ë” ìƒì„±
    Path("output").mkdir(exist_ok=True)
    print()

    # Extract
    print()
    print("[STEP 1/4] Extract PDF")
    print("-" * 70)
    text, metadata, pages = extract_pdf(pdf_path)
    if not text:
        print("[ERROR] Failed to extract text from PDF")
        return

    print(f"[OK] âœ“ Extracted {len(text):,} characters from {len(pages)} pages")
    print()

    # Chunk
    print("[STEP 2/4] Create chunks")
    print("-" * 70)
    chunks = chunk_text(text, chunk_size=5000)
    print(f"[OK] âœ“ Total chunks to translate: {len(chunks)}")
    print()

    # Translate
    print("[STEP 3/4] Translate with Claude API (ë³‘ë ¬ ì²˜ë¦¬)")
    print("-" * 70)
    translated_chunks = translate_chunks(chunks, "English", "Korean", api_key)

    if not translated_chunks:
        print("[ERROR] Translation failed")
        return

    print()
    
    # Generate markdown
    print("[STEP 4/4] Generate markdown")
    print("-" * 70)
    markdown = generate_markdown(pdf_path.stem, translated_chunks, text, len(pages))
    print()

    # output/ í´ë”ì— ì €ì¥
    print("[SAVING] Writing output file...")
    output_path = Path('output') / f'output_{pdf_path.stem}_translated.md'
    output_path.write_text(markdown, encoding='utf-8')
    print(f"[OK] âœ“ Output saved: {output_path.name}")
    print()

    # Summary
    print("=" * 70)
    print("[âœ“ SUMMARY]")
    print("=" * 70)
    print(f"  ğŸ“„ PDF File: {pdf_path.name}")
    print(f"  ğŸ“– Pages: {len(pages)}")
    print(f"  ğŸ“ Total Characters: {len(text):,}")
    print(f"  ğŸ“¦ Chunks Created: {len(chunks)}")
    print(f"  ğŸŒ Chunks Translated: {len(translated_chunks)}")
    print(f"  ğŸ’¾ Output File: {output_path.name}")
    print(f"  ğŸ“ Location: {output_path.absolute()}")
    print("=" * 70)
    print()
    print("[SUCCESS] âœ¨ Complete translation pipeline finished!")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"[ERROR] {e}")
        import traceback
        traceback.print_exc()
