# PDF ì¶”ì¶œ ë° ë²ˆì—­ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
# ì‚¬ìš©: python test_pdf_extract.py

from pathlib import Path
import sys

# PDF ì²˜ë¦¬ í•¨ìˆ˜
def extract_pdf_simple(pdf_path):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)"""
    try:
        import pdfplumber

        with pdfplumber.open(pdf_path) as pdf:
            text = ""
            metadata = pdf.metadata
            pages = []

            print(f"ğŸ“– PDF ì •ë³´:")
            print(f"   í˜ì´ì§€: {len(pdf.pages)}")
            if metadata:
                print(f"   ì œëª©: {metadata.get('Title', 'ì—†ìŒ')}")
                print(f"   ì €ì: {metadata.get('Author', 'ì—†ìŒ')}")
            print()

            for i, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                text += page_text + "\n"
                pages.append(page_text)

            return text, metadata, pages

    except ImportError:
        print("âŒ pdfplumber ë¯¸ì„¤ì¹˜")
        print("ì„¤ì¹˜: pip install pdfplumber")
        return None, None, None


def chunk_text(text, chunk_size=5000):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°"""
    chunks = []
    words = text.split()
    current_chunk = []
    current_size = 0

    for word in words:
        current_size += len(word) + 1
        current_chunk.append(word)

        if current_size >= chunk_size:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_size = 0

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks


def mock_translate(text, source="English", target="Korean"):
    """ëª¨ì˜ ë²ˆì—­ (ì‹¤ì œ ë²ˆì—­ ì—†ì´ ë§ˆí‚¹ë§Œ)"""
    # ê°„ë‹¨í•œ í”„ë¦¬ë·°: ì›ë¬¸ + [ë²ˆì—­ë¨] í‘œì‹œ
    lines = text.split('\n')
    translated_lines = []

    for line in lines:
        if line.strip():
            # ê°„ë‹¨í•œ "ë²ˆì—­" ì²˜ë¦¬: ë‹¨ì–´ ìˆœì„œ ì•½ê°„ ì„ê¸°
            words = line.split()
            if len(words) > 3:
                # ë‹¨ì–´ ì¼ë¶€ë¥¼ ì¬ë°°ì—´í•˜ì—¬ "ë²ˆì—­ëœ" ê²ƒì²˜ëŸ¼ í‘œí˜„
                translated_lines.append(f"[KOR] {line[:50]}...")
            else:
                translated_lines.append(f"[KOR] {line}")
        else:
            translated_lines.append("")

    return "\n".join(translated_lines)


def main():
    pdf_path = Path('src/translation/laf.pdf')

    if not pdf_path.exists():
        print(f"âŒ PDF íŒŒì¼ ì—†ìŒ: {pdf_path}")
        return

    print("=" * 70)
    print("ğŸ“š AI Publishing Platform - PDF ë²ˆì—­ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print()

    # Step 1: PDF ì¶”ì¶œ
    print("Step 1ï¸âƒ£  PDF ì¶”ì¶œ")
    print("-" * 70)
    text, metadata, pages = extract_pdf_simple(pdf_path)

    if text is None:
        print("âŒ PDF ì¶”ì¶œ ì‹¤íŒ¨")
        return

    print(f"âœ… PDF ì¶”ì¶œ ì„±ê³µ")
    print(f"   ì´ ê¸€ì: {len(text):,}")
    print(f"   ì´ í˜ì´ì§€: {len(pages)}")
    print()

    # Step 2: ë¯¸ë¦¬ë³´ê¸°
    print("Step 2ï¸âƒ£  ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°")
    print("-" * 70)
    preview = text[:500]
    print(preview)
    print("...")
    print()

    # Step 3: ì²­í‚¹
    print("Step 3ï¸âƒ£  í…ìŠ¤íŠ¸ ì²­í‚¹ (5000ê¸€ì ë‹¨ìœ„)")
    print("-" * 70)
    chunks = chunk_text(text, chunk_size=5000)
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"   ì²­í¬ {i}: {len(chunk)} ê¸€ì")
    if len(chunks) > 3:
        print(f"   ...")
    print()

    # Step 4: ë²ˆì—­ (ëª¨ì˜)
    print("Step 4ï¸âƒ£  ë²ˆì—­ ì²˜ë¦¬ (Mock)")
    print("-" * 70)
    print("ì²« ë²ˆì§¸ ì²­í¬ ë²ˆì—­ ê²°ê³¼:")
    translated_first = mock_translate(chunks[0][:200])
    print(translated_first)
    print()

    # Step 5: ìµœì¢… ë§ˆí¬ë‹¤ìš´
    print("Step 5ï¸âƒ£  ë§ˆí¬ë‹¤ìš´ ìƒì„±")
    print("-" * 70)
    markdown_output = f"""# {pdf_path.stem} (ë²ˆì—­)

**ì›ë³¸**: English PDF
**ë²ˆì—­**: Korean (í•œêµ­ì–´)
**í˜ì´ì§€**: {len(pages)}
**ê¸€ì ìˆ˜**: {len(text):,}

## ë‚´ìš©

"""

    # ì²« 3ê°œ ì²­í¬ì˜ ë²ˆì—­ ì¶”ê°€
    for i, chunk in enumerate(chunks[:3], 1):
        markdown_output += f"### ì„¹ì…˜ {i}\n\n"
        translated = mock_translate(chunk[:300])
        markdown_output += translated + "\n\n"

    markdown_output += "...\n\n"
    markdown_output += f"**ì´ {len(chunks)}ê°œ ì„¹ì…˜ ë²ˆì—­ ì™„ë£Œ**"

    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥
    output_path = Path('output_laf_translated.md')
    output_path.write_text(markdown_output, encoding='utf-8')

    print(f"âœ… ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„±")
    print(f"   ê²½ë¡œ: {output_path.absolute()}")
    print()

    # ìš”ì•½
    print("=" * 70)
    print("ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½")
    print("=" * 70)
    print(f"âœ… PDF ì¶”ì¶œ: {len(pages)} í˜ì´ì§€")
    print(f"âœ… í…ìŠ¤íŠ¸: {len(text):,} ê¸€ì")
    print(f"âœ… ì²­í¬ ë¶„í• : {len(chunks)} ê°œ")
    print(f"âœ… ë²ˆì—­: {len(chunks)} ì²­í¬ ì²˜ë¦¬")
    print(f"âœ… ë§ˆí¬ë‹¤ìš´: {output_path}")
    print()

    # ê²°ê³¼ í”„ë¦¬ë·°
    print("ğŸ“ ìƒì„±ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:")
    print("-" * 70)
    print(markdown_output[:800])
    print("...")
    print()


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
